# ANALIZDAN5
Отчет по лабораторной работе #5 выполнил(а):
- Лысиков Дмитрий Александрович
- РИ-220948

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.
Ход работы:
- В ходе выполнения работы я создал сцену в Unity и реализовал систему поиска объекта агентом. Для того чтобы мне это реализовать, мне понадобился скрипт RollerAgent.
- Проанализировав скрипт RollerAgent.cs, нашел в нем коэффициент корреляции.

### Коэффициент корреляции был найден в методе OnActionReceived.
![Снимок экрана 2023-12-07 141958](https://github.com/DmitryLysikov/ANALIZDAN5/assets/129677338/bbc6edb2-fe91-4d8c-81f9-2ea33597a6bd)

Переменная "distanceToTarget" является связью между агентом и целью агента, а коэффициент "1.42" является верхней границей этой переменной, влияющей на ход обучения объекта текущей задаче.
distanceToTarget - это такая переменная, которая представляет собой расстояние между текущей позицией объекта и целевой позицией.
![Снимок экрана 2023-12-07 142623](https://github.com/DmitryLysikov/ANALIZDAN5/assets/129677338/7a4a7ea6-9325-4c0b-a399-27d91576547d)

Из условного оператора if мы видим, что если объект агента достигает цели, а это distanceToTarget < 1.42f, то цель достигнута успешно, и агент получает вознаграждение. 
Однако, если агент опускается ниже определенной высоты, а это this.transform.localPosition.y < 0, то завершается эпизод, но без вознаграждения.

Вывод: чем меньше этот коэффициент, тем точнее будет обучаться модель, ведь расстояние, необходимое для получения вознаграждения, будет меньше. А в противном случае будут проблемы с обучением.
  
## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

![image](https://github.com/DmitryLysikov/ANALIZDAN5/assets/129677338/db8e85b8-9f7c-467f-9f0c-5db02240f8b8)

В данном коде я нашел 3 параметра, которые влияют на обучение модели.
  
- batch_size - параметр определяет велечину,  которая обрабатывается моделью за один проход обучения. Уменьшив значение до 4, обучение стало проводится менее стабильно.

- buffer_size - параметр размера буфера, который используется для хранения велечин и их обучения.
   
- learning_rate - параметр устанавливает скорость обучения, которая определяет скорость адаптации к данным. 

Я попытался изменить параметры и вот как это влияет, увеличение значения batch_size может ускорить обучение, но может привести к уменьшению точности модели. Увеличение значения buffer_size может ускорить обучение, но требует больше памяти. Увеличение значения learning_rate может ускорить обучение, но может привести к неустойчивому обучению.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Пример использования ML-агента, может быть в игре FIFA связанная с игрой против компьютера. Компьютер играя за игроков может обучаться финтам и блокированию соперника. Также в игре компьютер может подстраиваться под соперника проводя замены или меняя схему игры. 

![ebd1029b949a431296e9c6e7f9e9fa76](https://github.com/DmitryLysikov/ANALIZDAN5/assets/129677338/95f8a36c-7cce-4252-8139-7e2a60c65cf1)

Использование ML-Agent'а может быть предпочтительным, например, в следующих случаях:
1) Управление большим количеством действий.
2) Когда необходимо, чтобы агент автоматически обновлял свои стратегии в соответствии с новыми данными и опытом.
3) Вычисление тяжелых операций, например, расчет кратчайшего пути из точки А в точку В;

## Выводы

В ходе работы я познакомился с Unity ML-Agent, посмотрел примеры обучения и работы системы машинного обучения, проанализировал то, как они работают.
Кроме того, я думал по поводу того, в каких игровых задачах могут использоваться ML-агенты и искуственный интеллект в целом.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
